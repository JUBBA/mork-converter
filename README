Mork Converter
Version 2.0
July 9, 2010
Kevin Goodsell

Preliminary Info
----------------
This project began with a misunderstanding. I wanted to convert my
Thunderbird mailboxes to a different format, and I mistakenly thought
that this would require reading the Mork-formatted .msf files. It turns
out that a .msf (mail summary file) file is only an index. I'm not sure
how I missed this, but that's beside the point.

There are a few existing tools to read (some) Mork files, but they fail
to read .msf files. I started writing my own tool, partly to experiment
with Python parsing using PLY (Python Lex-Yacc). In spite of my
realization that I had little or no real use for this tool, I decided to
continue working on it until it could be released in the hope that it
might be useful to others.

License
-------
This is free software released under the GNU General Public License,
version 2. Read the COPYING file for details.

This license does not apply to files in the other/ directory, which are
not my work. You'll have to read those files (or contact the authors)
for license information.

Contact
-------
If you find this useful or discover a problem with it, please let me
know.

Email:
kevin-opensource@omegacrash.net

Prerequisites
-------------
* Python 2, version 2.4 or higher. Python 3 is not supported.
* PLY (Python Lex-Yacc). Version 3.3 was used to develop and test this
  version. 2.5 Was used for the previous version and will probably still
  work.

Basic Usage
-----------
To read the Mork file history.dat and output an XML file called
mork.xml, use:

  mork history.dat

To do the same, but output to a file called history.xml, use:

  mork --format=xml:out=history.xml history.dat

To output to a set of CSV (Comma-Separated Values) files, one for each
table and meta-table in the Mork database, use:

  mork --format=csv history.dat

This will write the output files into a directory called csvout. You can
also write a single file with a header before each table by doing this:

  mork --format=csv:singlefile history.dat

This will still use the name csvout, but it will be a file rather than a
directory.

For additional help, use:

  mork --help

About Mork Files
----------------
Mork is a general-purpose text-based database format created for use in
the Mozilla project. I know of five specific ways it has been used in
Mozilla and related projects (Firefox, Thunderbird):

* Browsing history (history.dat)
* Form auto-complete history (formhistory.dat)
* Address book (abook.mab, history.mab)
* Mailbox index (*.msf)
* Mail folder cache (panacea.dat)

Unfortunately this format is rather convoluted and very difficult to
read, making the data contained in the database practically inaccessible
by non-Mozilla applications. Recently sqlite databases have replaced
Mork databases for some uses, largely alleviating this problem.

Mork represents data in tables. Tables are uniquely identified by a
namespace and ID. Tables include a set of rows and metadata in the form
of a 'meta-table'. Rows are likewise identified by a namespace and ID,
and may also include metadata in the form of a 'meta-row', however I
have not encountered any Mork files that make use of meta-rows. If this
tool encounters a meta-row it will report a warning and otherwise ignore
it. Row data is a set of key-value pairs where the keys are referred to
as columns.

The output from this tool depends on the output filter selected with the
--format option, but will typically contain a set of tables
corresponding to the tables in the Mork file, each identified by the
table namespace and ID. Each table's meta-table is also included, and
may contain information useful for interpreting the table contents.
Meta-tables seem to always contain a 'k' field that identifies the
'kind' of the table, and a 's' field for the table 'status', which is
often simply '9'. A digit in the status gives the priority of the table.
A 'u' or a 'v' indicate that the table is unique or verbose,
respectively.

Rows in a table are not uniform. Columns that appear in one row may not
appear in others. This makes the output a little odd, especially in
formats like CSV that require uniform rows. The result is that all
columns appear in the table header, and each row will have empty fields
for the columns that are not actually present in the row.

Each row is also identified by a namespace and ID, which typically shows
up in the output (as row attributes in XML and as extra columns in CSV).
The namespace may tell you something about the type of data in the row,
but other than that this can largely be ignored.

Filters
-------
This converter includes a basic framework for "filters", which are used
for writing output (output filters) and for tweaking the database prior
to output (database filters).

Available filters are shown in the --help output. Information on how to
write filters can be found in the doc/ subdirectory.

Output filters are specified with the --format option, and only one
output filter may be provided on the command line. Available output
filters include 'xml' and 'csv', for writing XML and CSV files,
respectively.

Database filters are specified with the --filter option. You may specify
as many as you'd like, and they are applied in the order they are given
on the command line. Available database filters include:

* utf16: Converts UTF-16 fields to UTF-8. You can usually identify files
  that need this treatment by embedded zero bytes in the output file
  which may cause it to be identified as a binary file.

* times: Converts fields that represent times and dates to standard and
  readable time formats.

* nometa: Removes meta-tables from the database. This might come in
  handy for cases where the meta-tables do not contain useful
  information, and just get in the way of interpreting the output. Note,
  however, that the meta-tables might be necessary for other filters to
  function properly. Because of this, the 'nometa' filter should
  probably be the last filter applied.

* emptycells: Remove cells with empty values. This is particularly
  useful for XML output of address books because each address book entry
  tends to contain a lot of unused, empty fields. In XML these translate
  to empty (and useless) elements.

Do not apply the same filter more than once. In some cases it will do
nothing, but it some it will cause errors.

Known Issues
------------
* The Mork documentation I've been able to find is not quite adequate,
  so there's a significant amount of guess-work involved in the
  translation.
* Many errors are currently reported with Python tracebacks. This is not
  very user-friendly.
* The parsing is rather slow. This seems to be an inherent issue with
  PLY.
* Some Mork files include data that appears to be encoded in UTF-16
  mixed with ASCII or UTF-8 data. There's no indication (as far as I can
  tell) when a field is UTF-16. The 'utf16' filter attempts to correct
  this problem, but only applies to fields that are explicitly mentioned
  in the filter source. This list may be incomplete, causing some fields
  to be left in UTF-16.
* A similar warning applies for the 'times' filter: the fields to be
  converted have to be explicitly given in the filter source. In
  addition, I haven't verified the correctness of the resulting dates.
  It's possible that the fields in the Mork file don't represent times
  in the way that I think they do, as "UNIX time": seconds (or some
  other small time unit) since the epoch at January 1, 1970.
